---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 4: Features of Time Series Data"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      ratio: '16:9'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 11, fig.height = 7)
```


```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
library(Quandl)
```



# Stochastic process

.pull-left[
![](Art/timeseries.png)
]

.pull-right[
Time series is a realization of chronologically stored sequence of random variables. 

This sequence of random variables is referred to as the *stochastic process*. 

We don't observe the stochastic process. We only observe the times series, that is, one realization of each random variable in the stochastic process.

]

---

# Time series

.right-column[

We use subscript $1$ to denote the first element of the time series, subscript $2$ to denote the second element of the time series, and so on, until the last observed element of the time series, for which we use subscript $T$.

So, this time series is given by $\{y_1,y_2,\ldots,y_T\}$. 

More generally, a time series is a finite sample from an underlying doubly-infinite sequence: $\{\ldots,y_{-1},y_{0},y_1,\ldots,y_{T-1},y_{T},y_{T+1},\ldots\}$.

]

---


# Observed time series

.right-column[

![](figures/lecture4/tseries.png)

]

---

# We only observe a segment of the history

.right-column[
We certainly have no access to the future. So, an observed time series can only run as far as the present time. 

We tend to have limited access to the past. For example, we don't have the actual measurements of temperature from times when the thermometer was yet to be invented. We also cannot have data on Soviet-era unemployment rates because tHeRe wAS nO uNeMpLoYmEnt in the USSR.
]

---


# What we see is just one realization

.right-column[

![](figures/lecture4/stationary.png)

]

---

# History is an unique outcome of all possible outcomes

.right-column[
The November 2022 the U.S. unemployment rate was $3.4$ percent.

In a parallel universe - e.g., if we could re-run the sequence of macroeconomic events that led to this realization - the unemployment rate may have been higher or lower. 

But we don't get to re-write histories. The unemployment rate of 3.4 percent is the only value that will ever be known for this period. 

The same is true for all the preceding and succeeding periods. In other words, out of infinite possible time series runs that we could have observed, we only observe one such run.
]

---


# Stationarity

.right-column[
The random variables that comprise the stochastic process may or may not have the same distributions. 

When they have the same distributions, the stochastic process is *stationary*. Otherwise, the stochastic process is *nonstationary*. 

Intuitively, a stochastic process is (strictly) stationary if a time shift does not change its statistical properties. 

More formally, stationary implies that the joint density of $Y_r,\ldots,Y_{r+k}$ is the same as that of $Y_s,\ldots,Y_{s+k}$, for any integers $r \ne s$ and $k$. 
]

---


# Stationarity

.right-column[
Stationarity is an important feature, and the assumption, on which the time series econometrics heavily relies. Broadly defined, stationarity implies statistical stability of the series. 

Some time series are stationary as is. Others are not. Indeed, most economic time series are nonstationary.
]

---


# Stationarity

.right-column[
To get an idea of whether a stochastic process is stationary or nonstationary, we would need to compare the distributions, or joint distributions, of the random variables comprising the stochastic process.

It seems like a straightforward exercise. Except, it is not a feasible one.

This is because we never observe the distribution of $Y_t$. Rather, in each period, we only observe its one realization, $y_t$. 

When we observe a time series, we can not be too sure whether these are realizations of a stationary stochastic process or a nonstationary stochastic process.
]

---


# We obsere the time series, not the stochastic process

.right-column[

![](figures/lecture4/nonstationary.png)

]

---


# With long (enough) time series we can get the idea

.right-column[
That the same observed time series may be a manifestation of a stationary process or a nonstationary process may seem problematic, which it would have been had we only observed a very short time series with only a handful of observations. 

But in practice, luckily, we usually work with at least several dozen observations, often hundreds or thousands of observations. 

A long enough time series allows us to infer a lot more from the single realizations of random variables than what would seem to be plausible.
]


---

# Ergodicity

.right-column[
This is where *ergodicity*, in conjunction with stationarity, kicks in.

Intuitively, ergodicity implies independence of random variables that are sufficiently far apart from each other in the stochastic process.

An important practical benefit of it is that when the process is stationary and ergodic, the moments of the time series converge to the moments of the stochastic process as the sample size increases. 
]


---

# Ergodicity

.right-column[
So, the mean of the time series will be equal to the mean of the random variables comprising the stochastic process, the variance of the time series will be equal to the variance of the random variables comprising the stochastic process, etc. 

Ergodicity has the effect of the law of large numbers in time series analysis, in the sense that the distribution of a long enough time series is representative of the distribution of the random variables comprising the underlying stationary stochastic process. 
]

---


# White noise process

.right-column[
A simplest kind of the time series is the realisation of a stochastic process that is comprised of independent and identically distributed random variables with zero mean and constant variance: $Y_t \sim iid~N\left(0,\sigma^2\right)$. This is referred to as a *white noise* process. 

A stochastic process is a white noise process if: 
$$\begin{aligned}
		& \mathbb{E}(Y_t) = 0,\;~\forall~t\\
    & Var(Y_t) = \sigma^2,\;~\forall~t\\
    & Cov(Y_t,Y_{t-k}) = 0,\;~\forall~k \ne 0
	\end{aligned}$$

]

---


# White noise process

.right-column[
A simplest kind of the time series is the realisation of a stochastic process that is comprised of independent and identically distributed random variables with zero mean and constant variance: $Y_t \sim iid~N\left(0,\sigma^2\right)$. This is referred to as a *white noise* process. 

A stochastic process is a white noise process if: 
$$\begin{aligned}
		& \mathbb{E}(Y_t) = 0,\;~\forall~t\\
    & Var(Y_t) = \sigma^2,\;~\forall~t\\
    & Cov(Y_t,Y_{t-k}) = 0,\;~\forall~k \ne 0
	\end{aligned}$$

]

---


# White noise process

.right-column[

![](figures/lecture4/wn.png)

]

---


# Stationarity

.right-column[
Because in a white noise sequence observations are drawn from the same distribution, it is a *stationary* process. Indeed, a special type of stationary process insofar as it has time-invariant mean, variance, and covariance. 

For a stochastic process to be stationary, neither its mean needs to be equal to zero, it only needs to be constant over time, nor its covariances need to be equal to zero, they only need to be constant over time, though they may vary with $k$. 
]

---


# Stationarity

.right-column[
Thus, a time series is a realization of the stationary process (specifically of the weakly stationary process) if the mean and variance are independent of $t$, and the autocovariances are independent of $t$ for all $k$.
]

---





# Sometimes we need to transform data

.right-column[
Usual forms of transformation involve first-differencing, taking natural logarithm (log-transforming), or first differencing the log-transformed series (log-differencing), which are done to work with a suitable variable for the desired econometric analysis. 

The first-difference operator is denoted by $\Delta$, so that $\Delta y_t = y_t-y_{t-1}$. First-differencing helps remove a trend from the time series.
]

---


# Transformations can resolve issues

.right-column[
If an economic time series is characterized by an apparent exponential growth, by taking natural logarithms the time series 'flatten' and the fluctuations become proportional, which is one of the requirements for stationarity. The other requirement is constant mean (or no trend) which is achieved by first-differencing the log-transformed series. 

Thus, if a time series $\{y_t\}$ appear to be trending exponentially, the transformed series $\{\Delta\ln y_t\}$ should be stationary.
]

---


# Autocorrelation

.right-column[
White noise process assumes no correlation between $Y_t$ and $Y_{t-k}$ for any $k\neq0$. However, it is more of a norm than an exception, for a time series to be correlated, which is often the case in economic data.

Indeed, we often observe dependence among the temporally adjacent time series. In other words, $Y_t$ and $Y_{t-k}$ tend to be correlated for reasonably small values of $k$. 

Such correlations are referred to as *autocorrelations*, and are given by: $$\rho_k \equiv Corr(Y_t,Y_{t-k}) = \frac{Cov(Y_t,Y_{t-k})}{Var(Y_t)},\;~~k=1,2,\ldots$$ 
]

---


# Autocorrelation

.right-column[
We can illustrate the time series autocorrelations using *autocorrelogram*, which plots the sequence of autocorrelation coefficients against the lags at which these coefficients are obtained. 

Under the null of independence, the estimated autocorrelations, $\hat{\rho}_k$, are asymptotically standard normally distributed, $\sqrt{T}\hat{\rho}_k\xrightarrow{d}N(0,1)$, and thus: $\hat{\rho}_k \sim N(0,T^{-1})$

The approximate $(1-\alpha)$% confidence interval is bounded by $-z_{\alpha/2}T^{-1/2}\;\text{and}\;z_{\alpha/2}T^{-1/2}$
]

---


# Autocorrelogram

.right-column[

]

---




# Readings

.pull-left[
![](Art/todolist.png)
]

.pull-right[
Ubilava, [Chapter 2](https://davidubilava.com/forecasting/docs/features-of-time-series-data.html)

Gonzalez-Rivera, Chapter 3

Hyndman & Athanasopoulos, [2.8](https://otexts.com/fpp3/acf.html), [2.9](https://otexts.com/fpp3/wn.html), [3.1](https://otexts.com/fpp3/transformations.html)
]



