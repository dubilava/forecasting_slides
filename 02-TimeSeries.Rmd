---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 2: Features of Time Series Data"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 9, fig.height = 6)
```


```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
```



# Stochastic Process and Time Series

Time series are realizations of a chronologically stored sequence of random variables. This sequence of random variables is referred to as the *stochastic process*. 

Thus, a time series is a realisation of the stochastic process. 

We index time periods as $1,2,\ldots,T$, and denote the set of observations as $\{y_t: t=1,\ldots,T\}$. 

A time series is a finite sample from an underlying doubly-infinite sequence: $\{\ldots,y_{-1},y_{0},y_1,\ldots,y_{T-1},y_{T},y_{T+1},\ldots\}$.

---


# White Noise

The simplest kind of time series is comprised of realizations from an independent and identically distributed random variable with zero mean and constant variance: $\varepsilon_t \sim iid\left(0,\sigma^2\right)$. 

---


# White Noise

```{r echo=FALSE, message=FALSE}
n <- 120

set.seed(n)
y <- rnorm(n)

df <- data.frame(t=c(1:n),y=y)

ggplot(df,aes(x=t,y=y))+
  geom_line(size=1,col="coral")+
  labs(y=expression(paste(y[t],sep="")))+
  theme_classic()+
  theme(axis.title = element_text(size=18),axis.text = element_text(size=14))
```

---


# White Noise

Such time series are referred to as *white noise*. 

A time series, $y$, is a white noise process if: 
\begin{align*}
& E(y_t) = 0,\;~\forall~t\\
& Var(y_t) = \sigma^2,\;~\forall~t\\
& Cov(y_t,y_{t-k}) = 0,\;~\forall~k \ne 0
\end{align*}

Because each observation is drawn from the same distribution, white noise is a *stationary* time series. Indeed, it is a special type of stationary time series insofar as it has time-invariant mean, variance, and covariance. 

---


# Stationarity

Stationarity is an important feature, and the assumption, on which the time series analysis heavily relies.

For a covariance stationary (or, weakly stationary) time series, the mean doesn't need to be equal to zero, it only needs to be constant over time; nor covariances need to be equal to zero, they also only need to be constant over time, though they may vary with $k$. 

Thus, a time series, $\{y_t\}$ is stationary if the mean and variance are independent of $t$, and the autocovariances are independent of $t$ for all $k$.

---


# Autocorrelation

It is more of a norm, rather than an exception, for a time series to be correlated. 

Indeed, we often observe dependence among the temporally adjacent time series. That is, for reasonably small values of $k$, $y_t$ and $y_{t-k}$ tend to be correlated. 

Such correlations are referred to as *autocorrelations*, and are given by: $$\rho_k \equiv Corr(y_t,y_{t-k}) = \frac{Cov(y_t,y_{t-k})}{Var(y_t)},\;~~k=1,2,\ldots$$ 

---


# Autocorrelation

We can illustrate the time series autocorrelations using *autocorrelogram*, which plots the sequence of autocorrelation coefficients against the lags at which these coefficients are obtained. 

Under the null of independence, the estimated autocorrelations, $\hat{\rho}_k$, are asymptotically standard normally distributed, $\sqrt{T}\hat{\rho}_k\xrightarrow{d}N(0,1)$, and thus: $\hat{\rho}_k \sim N(0,T^{-1})$

The approximate $(1-\alpha)$% confidence interval is bounded by $-z_{\alpha/2}T^{-1/2}\;\text{and}\;z_{\alpha/2}T^{-1/2}$

---


# Autocorrelation

```{r echo=FALSE, message=FALSE}
maxlag <- 10
df <- data.frame(k=c(1:maxlag),rho=c(acf(y,plot=F)[1:10]$acf))

ggplot(df,aes(x=k,y=rho))+
  geom_segment(aes(xend=k,yend=0))+
  geom_hline(yintercept=0,size=.8)+
  geom_hline(yintercept=c(-1.96/sqrt(n),1.96/sqrt(n)),size=.5,linetype=5,col="steelblue")+
  scale_x_continuous(breaks=c(1:10),labels=c(1:10))+
  labs(x="k",y=expression(rho[k]))+
  coord_cartesian(ylim=c(-1,1))+
  theme_classic()+
  theme(axis.title = element_text(size=18),axis.text = element_text(size=14))
```

---


# Transformations

Sometimes we (have to) transform time series, either by first-differencing or by taking natural logarithms (or both), to work with a suitable variable for the desired econometric analysis. 

The first-difference operator is denoted by $\Delta$, so that $\Delta y_t = y_t-y_{t-1}$. First-differencing helps remove a linear trend from the time series.

---


# Transformations

If an economic time series is characterized by an apparent exponential growth (e.g., real GDP), by taking natural logarithms the time series 'flatten' and the fluctuations become proportionate. 

The next three graphs illustrate (i) a time series with an apparent exponential growth, (ii) the natural logarithms of this time series, and (iii) the first-differences of the log--transformed series.

---


```{r echo=FALSE, message=FALSE}
n <- 120
a <- 0.05
b <- 0.05
s <- 0.05
set.seed(n)
e <- rnorm(n,0,s)
dlny <- e
for(i in 2:n){
  dlny[i] <- .05+.05*dlny[i-1]+e[i]
}
lny <- cumsum(dlny)
y <- exp(lny)

df <- data.frame(t=c(1:n),y=y,lny=lny,dlny=dlny)
```

# Transformations

```{r echo=FALSE, message=FALSE}
ggplot(df,aes(x=t,y=y))+
  geom_line(size=1,color="coral")+
  labs(y=expression(paste(y[t],sep="")))+
  theme_classic()+
  theme(axis.title = element_text(size=18),axis.text = element_text(size=14),axis.text.y = element_text(angle=90,hjust=0.5))
```

---


# Transformations

```{r echo=FALSE, message=FALSE}
ggplot(df,aes(x=t,y=lny))+
  geom_line(size=1,color="coral")+
  labs(y=expression(paste("ln",y[t],sep="")))+
  theme_classic()+
  theme(axis.title = element_text(size=18),axis.text = element_text(size=14),axis.text.y = element_text(angle=90,hjust=0.5))
```

---


# Transformations

```{r echo=FALSE, message=FALSE}
ggplot(df,aes(x=t,y=dlny))+
  geom_line(size=1,color="coral")+
  labs(y=expression(paste(Delta,"ln",y[t],sep="")))+
  theme_classic()+
  theme(axis.title = element_text(size=18),axis.text = element_text(size=14),axis.text.y = element_text(angle=90,hjust=0.5))
```

---


# Evaluating Time Series Forecasts

To evaluate forecasts of a time series, $\{y_t\}$, with a total of $T$ observations, we divide the sample into two parts, the in-sample set with a total of $R$ observations, such that $R < T$ (typically, $R \approx 0.75T$), and the out-of-sample set. 

The goal is to recreate a 'genuine' forecasting environment.

For example, If we are interested in one-step-ahead forecast assessment, this way we will produce a sequence of forecasts: $\{y_{R+1|R},y_{R+2|{R+1}},\ldots,y_{T|{T-1}}\}$ for $\{Y_{R+1},Y_{R+2},\ldots,Y_{T}\}$.

Forecast errors, $e_{R+j} = y_{R+j} - y_{R+j|{R+j-1}}$, then can be computed for $j = 1,\ldots,T-R$.

---


# Evaluating Time Series Forecasts

Three forecasting schemes are available for a forecaster: *recursive*, *rolling*, and *fixed*.
- The recursive forecasting environment uses a sequence of expanding windows to update model estimates and the information set.
- The rolling forecasting environment uses a sequence of rolling windows of the same size to update model estimates and the information set.
- The fixed forecasting environment uses one fixed window for model estimates, and only updates the information set.

---


# Evaluating Time Series Forecasts

Forecast errors of a 'good' forecasting method will have the following properties:
- zero mean; otherwise, the forecasts are biased.
- no correlation with the forecasts; otherwise, there is information left that should be used in computing forecasts.
- no serial correlation among one-step-ahead forecast errors. Note that $k$-step-ahead forecasts, for $k>1$, can be, and usually are, serially correlated.

Any forecasting method that does not satisfy these properties has a potential to be improved. 

---


# Forecast Error Diagnostics

## Unbiasedness

Testing $E(e_{t+h|t})=0$. Set up a regression: $$e_{t+h|t} = \alpha+\upsilon_{t+h} \hspace{.5in} t = R,\ldots,T-h,$$
where $R$ is the estimation window size, $T$ is the sample size, and $h$ is the forecast horizon length. The null of zero-mean forecast error is equivalent of testing $H_0: \alpha = 0$ in the OLS setting. For $h$-step-ahead forecast errors, when $h>1$, autocorrelation consistent standard errors should be used.

---


# Forecast Error Diagnostics

## Efficiency

Testing $Cov(e_{t+h|t},y_{t+h|t})=0$. Set up a regression: $$e_{t+h|t} = \alpha + \beta y_{t+h|t} + \upsilon_{t+h} \hspace{.5in} t = R,\ldots,T-h.$$ The null of forecast error independence of the information set is equivalent of testing $H_0: \beta = 0$ in the OLS setting. For $h$-step-ahead forecast errors, when $h>1$, autocorrelation consistent standard errors should be used.

---


# Forecast Error Diagnostics

## No Autocorrelation

Testing $Cov(e_{t+1|t},e_{t|t-1})=0$. Set up a regression: $$e_{t+1|t} = \alpha + \gamma e_{t|t-1} + \upsilon_{t+1} \hspace{.5in} t = R+1,\ldots,T-1.$$ The null of no forecast error autocorrelation is equivalent of testing $H_0: \gamma = 0$ in the OLS setting.


---


# Readings

Gonzalez-Rivera, Chapter 3

Hyndman & Athanasopoulos, [Chapters 2, 3 & 4](https://otexts.com/fpp3/toolbox.html)


