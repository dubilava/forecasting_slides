<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Forecasting for Economics and Business</title>
    <meta charset="utf-8" />
    <meta name="author" content="David Ubilava" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Forecasting for Economics and Business
]
.subtitle[
## Lecture 7: Forecasting Temporally Dependent Seriesâ€”Autoregression
]
.author[
### David Ubilava
]
.date[
### University of Sydney
]

---





&lt;style&gt;
  .myTable td {
    padding: 5px 20px 5px 20px;
  }
&lt;/style&gt;




# Cycles

.pull-left[
![](Art/autoregression.png)
]

.pull-right[
A time series that exhibit a pattern of periodic fluctuations can be a cyclical series. 

Cycles can be *deterministic* or *stochastic*. 

Deterministic cycles can be modeled in a fashion similar to seasonality (e.g., using terms from a Fourier series). However, many time series, and especially economic time series, are better characterized by stochastic cycles.
]

---


# Cycles in (seasonally adjusted) unemployment rates

.right-figure[
![](figures/lecture7/unrate.png)
]


---


# Autoregressive stochastic cycle

.right-column[
A cycle is stochastic when it is generated by random variables. 

Of particular interest is the case when these random variables belong to the same stochastic process and are temporally dependent.

Realized values from such a process form a time series that can be approximated by an *autoregressive model*.
]

---


# Autoregressive model

.right-column[
In general terms, an autoregressive model of order `\(p\)`, `\(AR(p)\)`, has the following functional form:
`$$y_t = \alpha + \beta_1 y_{t-1}+\beta_2 y_{t-2}+ \cdots + \beta_p y_{t-p}+\varepsilon_t$$`
In this model, `\(\alpha\)` is a constant term, and `\(\beta_1,\ldots,\beta_p\)` are autoregressive parameters.

The sum of the autoregressive parameters depicts the persistence of the series. The larger is the persistence (i.e., closer it is to one), the longer it takes for the effect of a shock to dissolve.
]

---


# Autoregressive model

.right-column[
In practice, we don't know the values of the autoregressive parameters. So, we estimate those.

For that, we also need to select the lag order of the autoregressive model. We can do that using information criteria.
]

---


# Lag order selection using information criteria

.pull-left[

| k | AIC   | SIC   |
|---|:-----:|:-----:|
|1  | 3.197 | 3.227 |
|2  | 2.621 | 2.665 |
|3  | 2.627 | 2.687 |
|4  | 2.635 | 2.710 |
]

.pull-right[
Suppose we suspect that the lag order of the unemployment rates series is at most four. So, we estimate AR(1) through AR(4) and obtain information criteria (AIC and SIC). 

In this instance, both information criteria suggest that AR(2) is the model that best approximates the dynamic properties of the time series.
]

---


# Mean and variance of AR(1)

.right-column[
Let `\(\mu\)` be the mean of the time series. Under the assumption of covariance stationarity, `\(\mu\equiv\mathbb{E}\left(y_t\right),\;~~\forall~t.\)`

If we take the expectation of the AR(p) and solve for `\(\mu\)`, we obtain:
`$$\mu = \frac{\alpha}{1-\left(\beta_1+\ldots+\beta_p\right)}.$$`

Let `\(\gamma_0\)` be the variance of the time series. Under the assumption of covariance stationarity, `\(\gamma_0\equiv\mathbb{V}\left(y_t\right),\;~~\forall~t.\)`

`$$\gamma_0 = \mathbb{V}\left(y_t\right) = \mathbb{V}\left(\frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}\right) = \frac{\sigma_{\varepsilon}^2}{1-\beta_1^2}$$`
]

---


# Autocovariance of AR(1)

.right-column[
*Autocovariance*: `$$\gamma_k = Cov(y_t,y_{t-k}) = E[(y_t - \mu)(y_{t-k} - \mu)] = \mathbb{E}(y_t y_{t-k}) - \mu^2$$`

Multiply both sides of `\(AR(1)\)` with `\(y_{t-k}\)` and take the expectation: `$$\mathbb{E}(y_t y_{t-k}) = \alpha \mu + \beta_1 \mathbb{E}(y_{t-1}y_{t-k})$$`

Substitute in expressions of `\(\mathbb{E}(y_t y_{t-k})\)` and `\(\mathbb{E}(y_{t-1} y_{t-k})\)`, which is equivalent to `\(\mathbb{E}(y_{t} y_{t-k+1})\)`, to obtain: `$$\gamma_k = \beta_1\gamma_{k-1}$$`
]

---


# Autocorrelations of AR(1)

.right-column[
*Autocorrelation* (recall, `\(\rho_k = \gamma_k/\gamma_0\)`): `\(\rho_{k} = \beta_1\rho_{k-1}\)`

It then follows that:
`$$\begin{align}
\rho_1 &amp;= \beta_1\rho_0 = \beta_1 \notag \\
\rho_2 &amp;= \beta_1\rho_1 = \beta_1^2 \notag \\
&amp;\vdots \notag \\
\rho_k &amp;= \beta_1\rho_{k-1} = \beta_1^k
\end{align}$$`
]

---


# Autocorrelations of AR(1)

.right-column[
The autocorrelation, `\(\rho\)`, and partial autocorrelation, `\(\pi\)`, functions of the AR(1) process have three distinctive features:

- `\(\rho_1 = \pi_1 = \beta_1\)`. That is, the persistence parameter is also the autocorrelation and the partial autocorrelation coefficient.
- The autocorrelation function decreases exponentially toward zero, and the decay is faster when the persistence parameter is smaller.
- The partial autocorrelation function is characterized by only one spike `\(\pi_1 \neq 0\)`, and the remaining `\(\pi_k = 0\)`, `\(\forall k &gt; 1\)`.
]

---


# Autoregression of order p&amp;mdash;AR(p)

.right-column[
Most things about AR(1) can be generalized to any AR(p). 

AR(p) can be viewed as a bit more complex version of an AR(1). 
- For example, requirements on the parameters to ensure that an AR(p) is a covariance-stationary process become tedious as the order of autoregression increases (luckily, we don't need to worry too much about it).
]

---


# Autocorrelations of AR(p)

.right-column[
Generally, the autocorrelation and partial autocorrelation functions of the covariance-stationary `\(AR(p)\)` process have the following features:

- `\(\rho_1 = \pi_1\)`, and `\(\pi_p = \beta_p\)`.
- The autocorrelation function decreases toward zero, but in different fashion depending on the values of `\(\beta_1,\ldots,\beta_p\)`. Nonetheless, the decay is faster when the persistence measure is smaller.
- The partial autocorrelation function is characterized by the first `\(p\)` spikes `\(\pi_1 \neq 0,\ldots,\pi_p \neq 0\)`, and the remaining `\(\pi_k = 0\)`, `\(\forall k &gt; p\)`.
]

---


# Autocorrelogram of the unemployment rates

.right-figure[
![](figures/lecture7/unrate_ac.png)
]


---


# Partial autocorrelogram of the unemployment rates

.right-figure[
![](figures/lecture7/unrate_pac.png)
]


---




# Order selection in autoregression

.right-column[
Partial autocorrelations can give a hint about the lag order of the autoregression.

More formally, we can apply an information criterion (AIC or SIC) to select the order of autoregression.
]

---





# Autoregression of order one&amp;mdash;AR(1)

.right-column[
The simplest, i.e., the most parsimonious model in the family of autoregressive models, is the first-order autoregression: `$$y_t = \alpha + \beta_1 y_{t-1} + \varepsilon_t,$$` where `\(\alpha\)` is a constant term; `\(\beta_1\)` is the *persistence* parameter; and `\(\varepsilon_t\)` is a white noise process.

We only require `\(|\beta_1| &lt; 1\)` for an `\(AR(1)\)` process to be covariance stationary.
]

---


# Autoregression of order one&amp;mdash;AR(1)

.right-column[
Substitute recursively lagged dependent variables:
$$
`\begin{align}
y_t &amp;= \alpha + \beta_1 y_{t-1} + \varepsilon_t \notag \\
y_t &amp;= \alpha + \beta_1 (\alpha + \beta_1 y_{t-2} + \varepsilon_{t-1}) + \varepsilon_t \notag \\
&amp;= \alpha(1+\beta_1) + \beta_1^2 (\alpha + \beta_1 y_{t-3} + \varepsilon_{t-2}) + \beta_1\varepsilon_{t-1} + \varepsilon_t \notag \\
&amp;\vdots  \notag \\
&amp;= \alpha\sum_{i=0}^{k-1}\beta_1^i + \beta_1^k y_{t-k} + \sum_{i=0}^{k-1}\beta_1^i\varepsilon_{t-i}
\end{align}`
$$
]

---


# Autoregression of order one&amp;mdash;AR(1)

.right-column[
The end-result is a general linear process with geometrically declining coefficients. The `\(|\beta_1| &lt; 1\)` (i.e., covariance-stationarity) is required for convergence.

Then, as `\(k \to \infty\)`: `$$y_t = \frac{\alpha}{1-\beta_1} + \sum_{i=0}^{\infty}\beta_1^i\varepsilon_{t-i}$$`
]

---


# Point forecast from an autoregressive model

.right-column[
A one-step-ahead point forecast (assuming quadratic loss) is a conditional mean of the random variable in the next period: `$$\hat{y}_{t+1|t} = \mathbb{E}\left(y_{t+1}|\Omega_t;\mathbf{\theta}\right) = \hat{\alpha} + \hat{\beta}_1 y_{t} + \cdots  + \hat{\beta}_p y_{t-p+1}$$`

So, one-step-ahead point forecast is a linear function of the most recent observations, all of which are available (to a forecaster) at the time when the forecast is made, and the parameter estimates. So, we can readily generate such a forecast.

]

---

# Point forecast from an autoregressive model

.right-column[
To be able to make a two-step-ahead forecast in period `\(t\)`, we need to have observed data up to period `\(t+1\)`. In other words, we need to have observed `\(y_{t+1}\)`, which we haven't. Instead, we can use our best guess about `\(y_{t+1}\)`, which is `\(\hat{y}_{t+1|t}\)`

So, a two-step-ahead point forecast is a linear function of the one-step-ahead forecast, the most recent observations at the time when the forecast is made, and the parameter estimates.

]

---

# Point forecast from an autoregressive model

.right-column[
Generally, a h-step-ahead point forecast: `$$\hat{y}_{t+h|t} = \mathbb{E}\left(y_{t+h}|\Omega_t;\mathbf{\theta}\right) = \hat{\alpha} + \hat{\beta}_1 \hat{y}_{t+h-1|t} + \cdots + \hat{\beta}_p \hat{y}_{t+h-p|t},$$` where each `\(\hat{y}_{t+h-j|t}\)`, `\(j&lt;h\)`, is sequentially generated. 

When `\(j\ge h\)`, `\(\hat{y}_{t+h-j|t}\equiv y_{t+h-j}\)`.

So, to generate a forecast for horizon `\(h\)`, forecasts for horizons `\(1\)` through `\(h-1\)` need to be generated first. This is known as the *iterated* method of multistep forecasting. 

]

---



# Interval forecast from an autoregressive model

.right-column[
One-step-ahead forecast error: `$$e_{t+1|t} = y_{t+1} - y_{t+1|t} = \varepsilon_{t+1}$$`

One-step-ahead forecast variance: `$$\sigma_{t+1|t}^2 = \mathbb{E}(e_{t+1|t}^2) = \mathbb{E}(\varepsilon_{t+1}^2) = \sigma_{\varepsilon}^2$$`

One-step-ahead (95%) interval forecast: `$$y_{t+1|t} \pm 1.96\sigma_{\varepsilon}$$`
]

---


# Interval forecast from an autoregressive model

.right-column[
h-step-ahead forecast error: `$$e_{t+h|t} = \varepsilon_{t+h} + \beta_1 e_{t+h-1|t} + \cdots + \beta_p e_{t+h-p|t}$$`

h-step-ahead forecast variance: 
$$
`\begin{align}
\sigma_{t+h|t}^2 &amp;= \sigma_{\varepsilon}^2 + \sum_{i=1}^{p}\beta_i^2 Var(e_{t+h-i|t}) \\ 
&amp;+ 2\sum_{i \neq j}\beta_i\beta_j Cov(e_{t+h-i|t},e_{t+h-j|t})
\end{align}`
$$

h-step-ahead (95%) interval forecast: `\(y_{t+h|t} \pm 1.96\sigma_{t+h|t}\)`
]

---



# Iterated multistep forecasts of unemployment

.right-figure[
![](figures/lecture7/unrate_ite.png)
]


---

# Point forecast from an autoregressive model

.right-column[
We can, also, directly obtain multi-step-ahead point forecasts. 

Take an expression of a two-step-ahead point forecast, for example: `$$\hat{y}_{t+2|t} = \hat{\alpha} + \hat{\beta}_1 \hat{y}_{t+1|t} + \hat{\beta}_2 y_t + \cdots + \hat{\beta}_p y_{t+2-p}.$$`

If we substitute in an expression for the `\(\hat{y}_{t+1|t}\)`, and re-arrange terms, we obtain:
`$$\hat{y}_{t+2|t}=\tilde{\alpha} + \tilde{\beta}_1 y_{t} + \cdots  + \tilde{\beta}_p y_{t-p+1},$$` where `\(\tilde{\alpha}=\hat{\alpha}(1+\hat{\beta}_1)\)`, and `\(\tilde{\beta}_j=\hat{\beta}_1\hat{\beta}_j\)`, for all `\(j=1,\ldots,p\)`. 
]

---


# Point forecast from an autoregressive model

.right-column[
So, a way to obtain two-step-ahead forecast for an AR(p) model is to regress `\(y_t\)` on `\(y_{t-2},\ldots,y_{t-p-1}\)`, and then directly forecast `\(y_{t+2}\)` based on `\(y_{t},\ldots,y_{t-p}\)`, and the parameter estimates. 

This is the *direct* method of multistep forecasting, which can be extended to any forecast horizon.
]

---

# Direct multistep interval forecast

.right-column[
In the direct method of multistep forecasting, error terms are serially correlated (by construction). For example, note that in the direct two-step-ahead forecast, the error term is `\(u_t = \varepsilon_t+\beta_1\varepsilon_{t-1}\)`, where `\(\varepsilon_t\)` and  `\(\varepsilon_{t-1}\)` are the error terms of the AR(p) model. 

It then follows that: `$$\sigma^2_{u,2} = \mathbb{E}\left[(\varepsilon_t+\beta_1\varepsilon_{t-1})^2\right] = \sigma^2_{\varepsilon}(1+\beta_1^2).$$` 

Thus, when applying the direct method of forecasting, interval forecasts are obtained 'directly,' from the standard deviation of the residuals: `$$y_{t+h|t} \pm 1.96\sigma_{u,h}$$`
]

---


# Direct multistep forecasts of unemployment

.right-figure[
![](figures/lecture7/unrate_dir.png)
]


---


# Direct and iterated multistep forecasts

.right-column[
In summary, an `\(h\)`-step-ahead forecast from an AR(p) model, using:

- the iterated method is: `$$y_{t+h|t,i} = \alpha + \sum_{j=1}^{p}\beta_j y_{t+h-j|t,i},$$` where `\(y_{t+h-j|t,i}=y_{t+h-j}\)` when `\(h-j\le 0.\)`
- the direct method is: `$$y_{t+h|t,d} = \tilde{\alpha} + \sum_{j=1}^{p}\tilde{\beta}_j y_{t+1-j}.$$`
]

---


# Direct vs iterated multistep forecasts

.right-column[
The relative performance of the two forecasts, `\(y_{t+h|t,i}\)` and `\(y_{t+h|t,d}\)`, in terms of bias and efficiency depends on the bias and efficiency of the estimators of each method. 

If the model is correctly specified, both estimators are consistent, but the one-step-ahead model (which leads to the iterative method) is more efficient. Thus, in large samples, the iterative forecast can be expected to perform better than the direct forecast.

In the model is mis-specified&amp;mdash;hardly an unusual scenario&amp;mdash;the ranking of the relative performance may very well change.
]

---


# Key takeaways

.pull-left[
![](Art/todolist.png)
]

.pull-right[

]






    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>
<style>
.logo {
  background-image: url(forecasting-logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  bottom: .5em;
  left: 2em;
  width: 91px;
  height: 105px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
