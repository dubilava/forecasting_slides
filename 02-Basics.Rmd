---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 2: Basics of Forecasting"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 11, fig.height = 7)
```



```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
```

# A Forecast

A forecast is a random variable which has some distribution and, thus, moments.

A simplest form of a forecast is a point forecast (usually a mean of the distribution, but can be a median or, really, any quantile).

---


# A Forecast Error

A point forecast made in period $t$ for horizon $h$ can be denoted as $y_{t+h|t}$; this is our 'best guess', that is made in period $t$, about the actual realization of the random variable in period $t+h$, denoted by $y_{t+h}$.

The difference between the two is the forecast error. That is, $$e_{t+h|t} = y_{t+h} - y_{t+h|t}$$

---


# A Forecast Error

The more accurate is the forecast the smaller is the forecast error. 

Three types of uncertainty contribute to the forecast error:
$$\begin{aligned}
		e_{t+h|t} & = \big[y_{t+h}-E(y_{t+h}|\Omega_{t})\big]\;~~\text{(forecast uncertainty)}  \\
		& + \big[E(y_{t+h}|\Omega_{t}) - g(\Omega_{t};\theta)\big]\;~~\text{(model uncertainty)}  \\
		& + \big[g(\Omega_{t};\theta)-g(\Omega_{t};\hat{\theta})\big]\;~~\text{(parameter uncertainty)}
		\end{aligned}$$
		
- $\Omega_t$ denotes the information set available at the time when the forecast is made
- $g(\cdot)$ is a functional form of a model used to fit the data
- $\theta$ is a set of parameters of the model, and $\hat{\theta}$ are their estimates

---


# The Loss Function

Because uncertainty cannot be avoided, a forecaster is bound to commit forecast errors. 

The goal of the forecaster is to minimize the 'cost' associated with the forecast errors. This is achieved by minimizing the expected loss function.

A loss function, $L(e_{t+h|t})$, can take many different forms, but is should satisfy the following properties:
$$\begin{aligned}
		& L(e_{t+h|t}) = 0,\;~~\forall\;e_{t+h|t} = 0 \\
		& L(e_{t+h|t}) \geq 0,\;~~\forall\;e_{t+h|t} \neq 0 \\
		& L(e_{t+h|t}^{(i)}) > L(e_{t+h|t}^{(j)}),\;~~\forall\;|e_{t+h|t}^{(i)}| > |e_{t+h|t}^{(j)}|
		\end{aligned}$$

---


# The Loss Function

Two commonly used symmetric loss functions are *absolute* and *quadratic* loss functions:
$$\begin{aligned}
		& L{(e_{t+h|t})} = |e_{t+h|t}|\;~~\text{(absolute loss function)} \\
		& L{(e_{t+h|t})} = (e_{t+h|t})^2\;~~\text{(quadratic loss function)}
		\end{aligned}$$

The quadratic loss function is popular, partly because we typically select models based on 'in-sample' quadratic loss (i.e. by minimizing the sum of squared residuals).

---


# An Optimal Forecast

Optimal forecast is the forecast that minimizes the expected loss:
		$$\min_{y_{t+h|t}} E\left[L\left(e_{t+h|t}\right)\right] = \min_{y_{t+h|t}} E\left[L\left(y_{t+h}-y_{t+h|t}\right)\right]$$
		where the expected loss is given by:
		$$E\left[L\left(y_{t+h}-y_{t+h|t}\right)\right]=\int L\left(y_{t+h}-y_{t+h|t}\right) f(y_{t+h}|\Omega_t)dy$$


---


# An Optimal Forecast

We can assume that the conditional density is a normal density with mean $\mu_{t+h} \equiv E(y_{t+h})$, and variance $\sigma_{t+h}^2 \equiv Var(y_{t+h})$.

Under the assumption of the quadratic loss function:
$$\begin{aligned}
		E\left[L(e_{t+h|t})\right] & = E(e_{t+h|t}^2) = E(y_{t+h} - \hat{y}_{t+h|t})^2 \\
		& = E(y_{t+h}^2)-2E(y_{t+h})\hat{y}_{t+h|t} + \hat{y}_{t+h|t}^2
		\end{aligned}$$

By solving the optimization problem it follows that: $$\hat{y}_{t+h|t} = E(y_{t+h}) \equiv \mu_{t+h}$$

Thus, the optimal point forecast under the quadratic loss is the *mean*.

For reference, the optimal point forecast under absolute loss is the *median*.

---

# Simple Forecasting Methods

Recall that any guess may serve as a forecast, but an 'educated' guess is likely to be a better one.	We will now consider several simple methods for making an educated guess.

## The Average Method

The *average method* assumes that all future values of a given variable are equal to the mean of their historically observed values: $$E(y_{T+h}|\Omega_T) \equiv y_{T+h|T} = T^{-1}\sum_{t=1}^{T}y_t,\;~~\forall\;h=1,2,\ldots$$

---

# Simple Forecasting Methods

## The Naive Method

The *naive method* assumes that all future values of a given variable are equal to its most recent realization: $$E(y_{T+h}|\Omega_T) \equiv y_{T+h|T} = y_T,\;~~\forall\;h=1,2,\ldots$$

This method is derived from a *random walk* model, which assumes that the future is the present plus an unpredictable disturbance: $$y_{t} = y_{t-1}+\varepsilon_{t},\;~~\varepsilon_{t}\sim iid(0,\sigma^2)$$

---

# Simple Forecasting Methods

Alternatively, we can assume that the series are a random walk around a linear trend: $$y_t = y_{t-1}+\delta+\varepsilon_t,\;~~\varepsilon_t\sim iid(0,\sigma^2),$$ where $$\delta = E(\Delta y_t) = \frac{1}{T-1}\sum_{t=2}^{T}\Delta y_t = \frac{y_T - y_1}{T-1}$$

It then follows that: $$E(y_{T+h}|\Omega_T) \equiv y_{T+h|T} = y_T + h \delta = y_T + \frac{h(y_T - y_1)}{T-1}$$

---


# Measures of Forecast Accuracy

The most commonly applied accuracy measures are the mean absolute forecast error (MAFE) and the root mean squared forecast error (RMSFE):
$$\begin{aligned}
\text{MAFE}  = & \frac{1}{P}\sum_{i=1}^{P}|e_i|\\
\text{RMSFE} = & \sqrt{\frac{1}{P}\sum_{i=1}^{P}e_i^2}
\end{aligned}$$
where $P$ is the total number of out-of-sample forecasts.

---


# Measures of Forecast Accuracy

It must be noted that a model which fits the data well, may not necessarily forecast well. 
While a perfect fit can always be achieved by using a model with 'enough' parameters, such over-fitting can be seen as bad as failing to identify the systematic pattern in the data.

---


# Generating and Evaluating Forecasts

Forecast accuracy should only be determined by considering how well a model performs on data not used in estimation.

But to assess forecast accuracy we need access to the data, typically from future time periods, that was not used in estimation.

This leads to the so-called 'pseudo forecasting' routine. The routine involves slitting the available data into two segments referred to as 'in-sample' and 'out-of-sample'

- The in-sample segment of a series is also known as the 'estimation set' or the 'training set.'
- The out-of-sample segment of a series is also known as the 'hold-out set' or the 'test set.'

---


# Generating and Evaluating Forecasts

Thus, we make the so-called 'genuine' forecasts using only the information from the estimation set, and assess the accuracy of these forecasts in an out-of-sample setting.

Because forecasting is often performed in a time series context, the estimation set typically predates the hold-out set. In non-dynamic settings such chronological ordering may not be necessary, however.

---


# Readings

Hyndman & Athanasopoulos, [Sections from Chapter 5](https://otexts.com/fpp3/toolbox.html)

Gonzalez-Rivera, Chapter 4


