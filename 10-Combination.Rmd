---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 9: Forecast Combination"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 11, fig.height = 7)
```


```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
```

# Forecast Combination

Choosing the best (most adequate) model for forecasting also implies discarding all other considered methods.

Discarded forecasts, however, could possibly have some useful information, not available in the chosen forecast.

Thus, merely selecting the best model, from the available models, for forecasting may be a sub-optimal strategy.

An alternative strategy is to use some information from all forecasts, i.e., *forecast combination*.

---


# Forecast Combination
	
Several factors support the idea of forecast combination:

- The concept is intuitively appealing (more information is better).
- Many forecast combination techniques are computationally simple and easy to apply
- Empirical evidence strongly supports the idea of combining forecast to improve accuracy.
- The concept of forecast combination leads naturally to the concept of *forecast encompassing* - a valuable tool in forecast evaluation.

---


# Forecast Combination

Consider two forecasting methods (or models), $i$ and $j$, each respectively yielding one-step-ahead forecasts $y_{t+1|t,i}$ and $y_{t+1|t,j}$, and the associated forecast errors $e_{t+1,i} = y_{t+1}-y_{t+1|t,i}$ and $e_{t+1,j} = y_{t+1}-y_{t+1|t,j}$.

A combined forecast, $y_{t+1|t,c}$, is expressed as: $$y_{t+1|t,c} = (1-\omega)y_{t+1|t,i} + \omega y_{t+1|t,j},$$ where $0 \leq \omega \leq 1$ is a weight, and, thus, the combined forecast is a weighted average of the two individual forecasts.

Note: more than two forecasts can be combined, of course, but to keep the illustration simple, we will work with two forecasts here.

---


# Forecast Combination

A combined forecast error is: $$e_{t+1,c} = (1-\omega)e_{t+1,i} + \omega e_{t+1,j}$$

The mean of the combined forecast error (under the assumption of forecast error unbiasedness) is zero: $$E\left(e_{t+1,c}\right) = E\left[(1-\omega)e_{t+1,i} + \omega e_{t+1,j}\right] = 0$$

The variance of the combined forecast error is: $$Var\left(e_{t+1,c}\right) = (1-\omega)^2 \sigma_i^2 + \omega^2  \sigma_j^2 + 2\omega(1-\omega)\rho\sigma_i\sigma_j,$$ where $\sigma_i$ and $\sigma_j$ are the standard deviations of the forecast errors from models $i$ and $j$, and $\rho$ is a correlation between these two forecast errors.

---


# Optimal Weights for Combination

A simple optimization routine yields an optimal weight (which minimizes the combined forecast error variance): $$\omega^* = \frac{\sigma_i^2-\rho\sigma_i\sigma_j}{\sigma_i^2+\sigma_j^2-2\rho\sigma_i\sigma_j}$$

Substitute $\omega^*$ in place of $\omega$ in the foregoing equation to obtain: $$Var\left[e_{t+1,c}(\omega^*)\right] = \sigma_c^2(\omega^*) = \frac{\sigma_i^2\sigma_j^2(1-\rho^2)}{\sigma_i^2+\sigma_j^2-2\rho\sigma_i\sigma_j}$$


It can be shown that $\sigma_c^2(\omega^*) \leq \min\{\sigma_i^2,\sigma_j^2\}$; that is, by combining forecasts we are not making things worse (so long as we use *optimal* weights).

---


# Optimal Weights for Combination

## Case 1: $\sigma_i = \sigma_j = \sigma$.

Suppose the individual forecasts are equally accurate, then the combined forecast error variance reduces to: $$\sigma_c^2(\omega^*) = \frac{\sigma^2(1+\rho)}{2} \leq \sigma^2$$

The equation shows there are diversification gains even when the forecasts are equally accurate (unless the forecasts are perfectly correlated, in which case there are no additional benefits from combination).

---


# Optimal Weights for Combination

## Case 2: $\rho=0$.

Suppose the forecast errors are uncorrelated, then the sample estimator of $\omega^*$ is given by: $$\omega^* = \frac{\sigma_i^2}{\sigma_i^2+\sigma_j^2} = \frac{\sigma_j^{-2}}{\sigma_i^{-2}+\sigma_j^{-2}}$$

Thus, weights attached to forecasts are inversely proportional to their variance.

---


# Optimal Weights for Combination

## Case 3: $\sigma_i = \sigma_j = \sigma$ and $\rho=0$.

Suppose the individual forecasts are equally accurate and the forecast errors are uncorrelated, then the sample estimator of $\omega^*$ reduces to $0.5$, resulting in the equal-weighted forecast combination: $$y_{t+1|t,c} = 0.5y_{t+1|t,i} + 0.5y_{t+1|t,j}$$

---


# Optimal Weights for Combination

In practice $\sigma_i$, $\sigma_j$, and $\rho$ are unknown.

If we work with $P$ out-of-sample forecasts, the sample estimator of $\omega^*$ is: $$\hat{\omega}^* = \frac{\hat{\sigma}_i^2-\hat{\sigma}_{ij}}{\hat{\sigma}_i^2+\hat{\sigma}_j^2-2\hat{\sigma}_{ij}},$$ where $\hat{\sigma}_i^2 = \frac{1}{P-1}\sum_{t=R}^{R+P-1}{e_{t+1,i}^2}$ and $\hat{\sigma}_j^2 = \frac{1}{P-1}\sum_{t=R}^{R+P-1}{e_{t+1,j}^2}$ are sample forecast error variances, and $\hat{\sigma}_{ij}=\frac{1}{P-1}\sum_{t=R}^{R+P-1}{e_{t+1,i}e_{t+1,j}}$ is a sample forecast error covariance. 

---


# Forecast Encompassing

A special case of forecast combination is when $\omega=0$ (or when $\omega=1$). Such an outcome (of the optimal weights) is known as forecast encompassing.

It is said that $y_{t+1|t,i}$ encompasses $y_{t+1|t,j}$, when given that the former is available, the latter provides no additional useful information.

This is equivalent of testing the null hypothesis of $\omega=0$ in the combined forecast error equation, which, after rearranging terms, yields the following regression: $$e_{t+1,i} = \omega\left(e_{t+1,i}-e_{t+1,j}\right)+\varepsilon_{t+1},\;~~t=R,\ldots,R+P-1$$ where $\varepsilon_{t+1}\equiv e_{t+1,c}$, and where $R$ is the size of the (forst) estimation window, and $P$ is the number of out-of-sample forecasts generated..

---


# Forecast Encompassing

Alternatively, the forecast encompassing hypothesis is equivalent of testing the null hypothesis of $\omega=0$ in the following regression:  $$e_{i,t+1|t} = \omega y_{j,t+1|t}+\varepsilon_{t+1}$$

That is, the information set from the second method should not be of any value to (i.e. should not be correlated with) the forecast error from the first method.

---


# Optimal Weights for Combination

The optimal weight has a straightforward interpretation in a regression setting. Rewrite the combined forecast equation as: $$y_{t+1} = (1-\omega)y_{i,t+1|t} + \omega y_{j,t+1|t} + e_{c,t+1|t}.$$ Note that the equation is simply the least squares estimator of $\omega$. Thus, we can estimate weights and test the forecast encompassing hypothesis by regressing the realized value on individual forecasts as follows: $$y_{t+1} = \alpha + \beta_1 y_{i,t+1|t} + \beta_2 y_{j,t+1|t} + e_{c,t+1|t}$$

In this regression setting, the test of the null hypothesis of $\omega=0$ is equivalent to the test of the null hypothesis of $\beta_2=0$. We can, in fact, test a joint hypothesis that $\{\alpha,\beta_1,\beta_2\} = \{0,1,0\}$.

---

# Readings

Gonzalez-Rivera, Chapter 9



