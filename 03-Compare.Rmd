---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 3: Combining and Comparing Forecasts"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      ratio: '16:9'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 11, fig.height = 7)
```


```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
library(data.table)
library(lmtest)
library(sandwich)
```

# We may select the best using in-sample measures

.pull-left[
![](Art/evaluation.png)
]

.pull-right[
We often have several forecasts, each generated from a specific model or using a specific method.

The challenge, then, is to identify the most accurate among these.

One way to do so is using in-sample goodness of fit measures (e.g., AIC or SIC). 
]

---

# We may select the best using out-of-sample measures

.right-column[
Another way, which may be viewed as being more sensible, at least from the standpoint of a forecaster, is by evaluating forecasts in out-of-sample setting.

Recall that models with the best in-sample fit don't necessarily produce the best out-of-sample forecasts. While a better in-sample fit can be obtained by incorporating additional parameters in the model, the more complex models can lead to sub-optimal forecasts as they extrapolate the estimated parameter uncertainty into the forecasts.
]

---


# Comparing forecasts in an out-of-sample setting

.right-column[
Thus far we have applied the following algorithm to identify 'the best' among the competing forecasts:

- Decide on a loss function (e.g., quadratic loss).
- Obtain forecasts, the forecast errors, and the corresponding sample expected loss (e.g., root mean squared forecast error) for each model or method in consideration.
- Rank the models according to their sample expected loss values.
- Select the model with the lowest sample expected loss.
]

---


# Statistically significantly different

.right-column[
But the loss function is a function of a random variable, and in practice we deal with sample information, so sampling variation needs to be taken into the account.

Statistical methods of evaluation are, therefore, desirable.

Here we will cover two tests for the hypothesis that two forecasts are equivalent, in the sense that the associated loss difference is not statistically significantly different from zero.
]

---


# Loss differential

.right-column[
Consider a time series of length $T$. 

Suppose $h$-step-ahead forecasts for periods $R+h$ through $T$ have been generated from two competing models denoted by $i$ and $j$ and yielding $y_{i,t+h|t}$ and $y_{j,t+h|t}$, for all $t=R,\ldots,T-h$, with corresponding forecast errors: $e_{i,t+h}$ and $e_{j,t+h}$.
]

---


# Loss differential

.right-column[
The null hypothesis of equal predictive ability can be given in terms of the unconditional expectation of the loss difference: $$H_0: \mathbb{E}\left[d(e_{t+h})\right] = 0,$$ where, assuming quadratic loss, for example: $$d(e_{t+h}) = e_{i,t+h}^2-e_{j,t+h}^2.$$
]

---


# Morgan-Granger-Newbold test

.right-column[
The Morgan-Granger-Newbold (MGN) test is based on auxiliary variables: $u_{a,t+h} = e_{i,t+h}-e_{j,t+h}$ and $u_{b,t+h} = e_{i,t+h}+e_{j,t+h}$. It then follows that: $$\mathbb{E}(u_1,u_2) = MSFE(i)-MSFE(j).$$ Thus, the hypothesis of interest is equivalent to testing whether the two auxiliary variables are correlated.
]

---


# Morgan-Granger-Newbold test

.right-column[
The MGN test statistic is t-distributed with $P=1$ degrees of freedom, where $P$ is the total number of forecasts: $$MGN=\frac{r}{\sqrt{(P-1)^{-1}(1-r^2)}}\sim t_{P-1},$$ where $$r=\frac{\sum_{t=R}^{T-h}{u_{a,t+h}u_{b,t+h}}}{\sqrt{\sum_{t=R}^{T-h}{u_{a,t+h}^2}\sum_{t=R}^{T-h}{u_{b,t+h}^2}}}$$
]

---


# Morgan-Granger-Newbold test

.right-column[
The MGN test relies on the assumptions that forecast errors of the two forecasts to be compared, are unbiased, normally distributed, and uncorrelated (with each other). 

These are rather strict requirements that are, often, violated in empirical applications.
]

---


# Diebold-Mariano test

.right-column[
The Diebold-Mariano (DM) test relaxes the aforementioned requirements on the forecast errors. 

The DM test statistic is: $$DM=\frac{\bar{d}}{\sqrt{\sigma_d^2/P}} \sim N(0,1),$$ where $\bar{d}=P^{-1}\sum_{t=1}^{P}d(e_{t+h})$. 
]
		
---


# Diebold-Mariano test

.right-column[
A modified version of the DM statistic, due to Harvey, Leybourne, and Newbold (1998), addresses the finite sample properties of the test, so that: $$\sqrt{\frac{P+1-2h+P^{-1}h(h-1)}{P}}DM\sim t_{P-1},$$ where $t_{P-1}$ is a Student t distribution with $P-1$ degrees of freedom.
]

---


# Testing in a regression setting

.right-column[
In practice, the test of equal predictive ability can be applied within the framework of a regression model: $$d(e_{t+h}) = \delta + \upsilon_{t+h} \hspace{.5in} t = R,\ldots,T-h.$$

The null of equal predictive ability is equivalent of testing $H_0: \delta = 0$.

Because $d(e_{t+h})$ may be serially correlated, an autocorrelation consistent standard errors should be used for inference.
]

---


# Combining forecasts

.right-column[
By choosing the most accurate of the forecasts, we discard all others.

But other forecasts may not be completely useless. They could potentially contain useful information above and beyond of what the most accurate forecast may offer.

Thus, merely selecting the best model may be a sub-optimal strategy.

An optimal strategy may be to use some information from all forecasts, i.e., *forecast combination*.
]

---


# Why combine?
	
.right-column[
Several factors support the idea of forecast combination:

- The concept is intuitively appealing;
- The method is computationally simple;
- The outcome is surprisingly good.
]

---


# How it works?

.right-column[
Consider two forecasting methods (or models), $i$ and $j$, each respectively yielding $h$-step-ahead forecasts $y_{i,t+h|t}$ and $y_{j,t+h|t}$, and the associated forecast errors $e_{i,t+h} = y_{t+h}-y_{i,t+h|t}$ and $e_{j,t+h} = y_{t+h}-y_{j,t+h|t}$.

A combined forecast, $y_{c,t+h|t}$, is expressed as: $$y_{c,t+h|t} = (1-w)y_{i,t+h|t} + w y_{j,t+h|t},$$ where $0 \leq w \leq 1$ is a weight. Thus, a combined forecast is the weighted average of the two individual forecasts.
]

---


# A combined forecast error mean

.right-column[
A combined forecast error is the weighted average of the two individual forecast errors: $$e_{c,t+h} = (1-w)e_{i,t+h} + w e_{j,t+h}$$

The mean of a combined forecast error (under the assumption of forecast error unbiasedness) is zero: $$\mathbb{E}\left(e_{c,t+h}\right) = \mathbb{E}\left[(1-w)e_{i,t+h} + w e_{j,t+h}\right] = 0$$
]

---


# A combined forecast error variance

.right-column[
The variance of a combined forecast error is: $$Var\left(e_{c,t+h}\right) = (1-w)^2 \sigma_i^2 + w^2  \sigma_j^2 + 2w(1-w)\rho\sigma_i\sigma_j,$$ where $\sigma_i$ and $\sigma_j$ are the standard deviations of the forecast errors from models $i$ and $j$, and $\rho$ is a correlation between these two forecast errors.
]

---


# The optimal weight

.right-column[
Taking the derivative of the variance of a combined forecast error, and equating it to zero yields an optimal weight (which minimizes the combined forecast error variance): $$w^* = \frac{\sigma_i^2-\rho\sigma_i\sigma_j}{\sigma_i^2+\sigma_j^2-2\rho\sigma_i\sigma_j}$$
]

---


# At least as efficient as individual forecasts

.right-column[
Substitute $w^*$ in place of $w$ in the formula for variance to obtain: $$Var\left[e_{c,t+1}(w^*)\right] = \sigma_c^2(w^*) = \frac{\sigma_i^2\sigma_j^2(1-\rho^2)}{\sigma_i^2+\sigma_j^2-2\rho\sigma_i\sigma_j}$$


It can be shown that $\sigma_c^2(w^*) \leq \min\{\sigma_i^2,\sigma_j^2\}$. That is to say that by combining forecasts we are not making things worse (so long as we use *optimal* weights).
]

---


# Combination Weights&mdash;Special Cases

## Case 1: $\sigma_i = \sigma_j = \sigma$.

.right-column[
Suppose the individual forecasts are equally accurate, then the combined forecast error variance reduces to: $$\sigma_c^2(w^*) = \frac{\sigma^2(1+\rho)}{2} \leq \sigma^2$$

The equation shows there are diversification gains even when the forecasts are equally accurate (unless the forecasts are perfectly correlated, in which case there are no gains from combination).
]

---


# Combination Weights&mdash;Special Cases

## Case 2: $\rho=0$.

.right-column[
Suppose the forecast errors are uncorrelated, then the sample estimator of $w^*$ is given by: $$w^* = \frac{\sigma_i^2}{\sigma_i^2+\sigma_j^2} = \frac{\sigma_j^{-2}}{\sigma_i^{-2}+\sigma_j^{-2}}$$

Thus, the weights attached to forecasts are inversely proportional to the variance of these forecasts.
]

---


# Combination Weights&mdash;Special Cases

## Case 3: $\sigma_i = \sigma_j = \sigma$ and $\rho=0$.

.right-column[
Suppose the individual forecasts are equally accurate and the forecast errors are uncorrelated, then the sample estimator of $w^*$ reduces to $0.5$, resulting in the equal-weighted forecast combination: $$y_{c,t+h|t} = 0.5y_{i,t+h|t} + 0.5y_{j,t+h|t}$$
]

---


# The sample optimal weight

.right-column[
In practice $\sigma_i$, $\sigma_j$, and $\rho$ are unknown.

The sample estimator of $w^*$ is: $$\hat{w}^* = \frac{\hat{\sigma}_i^2-\hat{\sigma}_{ij}}{\hat{\sigma}_i^2+\hat{\sigma}_j^2-2\hat{\sigma}_{ij}},$$ where $\hat{\sigma}_i^2 = \frac{1}{P-1}\sum_{t=R}^{R+P-1}{e_{i,t+1}^2}$ and $\hat{\sigma}_j^2 = \frac{1}{P-1}\sum_{t=R}^{R+P-1}{e_{j,t+1}^2}$ are sample forecast error variances, and $\hat{\sigma}_{ij}=\frac{1}{P-1}\sum_{t=R}^{R+P-1}{e_{i,t+1}e_{j,t+1}}$ is a sample forecast error covariance, where $P$ denotes the number of out-of-sample forecasts. 
]

---


# The optimal weight in a regression setting

.right-column[
The optimal weight has a straightforward interpretation in a regression setting. Consider the combined forecast equation as: $$y_{t+1} = (1-w)y_{i,t+h|t} + w y_{j,t+h|t} + \varepsilon_{t+1},$$ where $\varepsilon_{t+1}\equiv e_{c,t+1}$. We can re-arrange the equation so that: $$e_{i,t+1} = w (y_{j,t+h|t}-y_{i,t+h|t}) + \varepsilon_{t+1},$$ where $w$ is obtained by estimating a linear regression with an intercept restricted to zero.
]

---

# The optimal weight in a regression setting

.right-column[
Alternatively, we can estimate a variant of the combined forecast equation: $$y_{t+1} = \alpha+\beta_i y_{i,t+h|t} + \beta_j y_{j,t+h|t} + \varepsilon_{t+1},$$ which relaxes the assumption of forecast unbiasedness, as well as of weights adding up to one or, indeed, of non-negative weights.
]

---


# Forecast encompassing

.right-column[
A special case of forecast combination is when $w=0$. Such an outcome (of the optimal weights) is known as forecast encompassing.

It is said that $y_{i,t+h|t}$ encompasses $y_{j,t+h|t}$, when given that the former is available, the latter provides no additional useful information.
]

---


# Forecast encompassing

.right-column[
This is equivalent of testing the null hypothesis of $w=0$ in the combined forecast error equation, which, after rearranging terms, yields the following regression: $$e_{i,t+1} = w\left(e_{i,t+1}-e_{j,t+1}\right)+\varepsilon_{t+1},\;~~t=R,\ldots,R+P-1$$ where $\varepsilon_{t+1}\equiv e_{c,t+1}$, and where $R$ is the size of the (first) estimation window, and $P$ is the number of out-of-sample forecasts generated.
]

---


# Forecast encompassing in a regression setting

.right-column[
We can test for the forecast encompassing by regressing the realized value on individual forecasts: $$y_{t+1} = \alpha + \beta_1 y_{i,t+h|t} + \beta_2 y_{j,t+h|t} + \varepsilon_{t+1},$$ and testing the null hypothesis that $\beta_2=0$, given that $\beta_1=1$. 
]

---


# Readings

.pull-left[
![](Art/todolist.png)
]

.pull-right[
Gonzalez-Rivera, Chapter 9 Sections 2 and 3
]
