<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Forecasting for Economics and Business</title>
    <meta charset="utf-8" />
    <meta name="author" content="David Ubilava" />
    <script src="libs/header-attrs-2.23/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Forecasting for Economics and Business
]
.subtitle[
## Lecture 3: Combining and Comparing Forecasts
]
.author[
### David Ubilava
]
.date[
### University of Sydney
]

---








# Comparing forecasts using in-sample measures

.pull-left[
![](Art/evaluation.png)
]

.pull-right[
We often have several forecasts, each generated from a specific model or using a specific method.

The challenge, then, is to identify the most accurate among these.

One way of doing it is by using in-sample goodness of fit measures. 
]

---

# In-sample goodness of fit measures

.right-column[
Recall the most frequently used (and often abused) R-squared: `$$R^2 = 1-\frac{\sum_{t=1}^{T}\hat{e}_t^2}{\sum_{t=1}^{T}(y_t - \bar{y})^2}$$`

Adjusted R-squared accounts for the loss in degrees of freedom: `$$\bar{R}^2 = 1-\frac{\sum_{t=1}^{T}\hat{e}_t^2}{\sum_{t=1}^{T}(y_t - \bar{y})^2}\left(\frac{T-1}{T-k}\right),$$` where `\(k\)` denotes the number of estimated parameters.
]

---


# In-sample goodness of fit measures

.right-column[
The adjustment made to the R-squared might not be 'enough' so select a 'good' forecasting model, however.

Information criteria penalize for the loss in degrees of freedom more 'harshly' than the adjusted R-squared:
`$$\begin{aligned}
AIC &amp; = \ln{\left(\sum_{t=1}^{T}{\hat{e}_t^2}\right)} + 2\frac{k}{T} \\
SIC &amp; = \ln{\left(\sum_{t=1}^{T}{\hat{e}_t^2}\right)} + \ln{T}\frac{k}{T}
\end{aligned}$$`
]

---


# In-sample goodness of fit measures

.right-column[
Things to remember about the information criteria:
- Less is better.
- Relative (not absolute) values of the criteria matter.
- SIC selects a more parsimonious model than AIC.
- The measures are used to compare fit, so long as the dependent variable is the same across the models.
]

---



# Comparing forecasts using out-of-sample measures

.right-column[
Another way of doing it, which may be viewed as being more sensible, at least from a forecaster's perspective, is by evaluating forecasts in an out-of-sample environment.

Recall that models with the best in-sample fit are not guaranteed to necessarily produce the most accurate forecasts. 
]

---

# A snapshot of multi-step-ahead El Nino forecasts

.pull-right[

![](figures/lecture3/model-predictions-of-enso.png)

]

---

# Historical data of one-step-ahead El Nino forecasts

.right-column[

![](figures/lecture3/historical-enso.png)

]

---



# Selecting based on a forecast accuracy measure

.right-column[
Thus far we have implied the following "algorithm" for selecting the most accurate among available forecasts:

- Decide on a loss function (e.g., quadratic loss).
- Obtain forecasts, the forecast errors, and the corresponding sample expected loss (e.g., root mean squared forecast error) for each model or method in consideration.
- Rank the models according to their sample expected loss values.
- Select the model with the lowest sample expected loss.
]

---

# Ranking of the models of El Nino forecasts

.right-column[

| Model     | MAFE  | RMSFE |
|-----------|:-----:|:-----:|
| ECMWF     | 0.193 | 0.249 |
| JMA       | 0.271 | 0.326 |
| CPC MRKOV | 0.297 | 0.362 |
| KMA SNU   | 0.314 | 0.376 |
| CPC CA    | 0.300 | 0.377 |
| CSU CLIPR | 0.311 | 0.392 |
| LDEO      | 0.310 | 0.395 |
| AUS/POAMA | 0.321 | 0.417 |
]

---


# Are they statistically significantly different?

.right-column[
But the loss function is a function of a random variable, and in practice we deal with sample information, so sampling variation needs to be taken into the account.

Statistical methods of evaluation are, therefore, desirable.
]

---


# Forecast errors from two competing models

.right-column[
Consider a time series of length `\(T\)`. 

Suppose `\(h\)`-step-ahead forecasts for periods `\(R+h\)` through `\(T\)` have been generated from two competing models denoted by `\(i\)` and `\(j\)` and yielding `\(y_{i,t+h|t}\)` and `\(y_{j,t+h|t}\)`, for all `\(t=R,\ldots,T-h\)`, with corresponding forecast errors: `\(e_{i,t+h}\)` and `\(e_{j,t+h}\)`.
]

---


# Loss differential

.right-column[
The null hypothesis of equal predictive ability can be given in terms of the unconditional expectation of the loss differential: `$$H_0: \mathbb{E}\left[d(e_{t+h})\right] = 0,$$` where, assuming quadratic loss, `$$d(e_{t+h}) = e_{i,t+h}^2-e_{j,t+h}^2.$$`
]

---

# Diebold-Mariano test

.right-column[
The Diebold-Mariano (DM) test relaxes the aforementioned requirements on the forecast errors. 

The DM test statistic is: `$$DM=\frac{\bar{d}}{\sqrt{\sigma_d^2/P}} \sim N(0,1),$$` where `\(\bar{d}=P^{-1}\sum_{t=1}^{P}d(e_{t+h})\)`, and where `\(P\)` is the total number of forecasts. 
]
		
---


# Modified Diebold-Mariano test

.right-column[
A modified version of the DM statistic, due to Harvey, Leybourne, and Newbold (1998), addresses the finite sample properties of the test, so that: `$$\sqrt{\frac{P+1-2h+P^{-1}h(h-1)}{P}}DM\sim t_{P-1},$$` where `\(t_{P-1}\)` is a Student t distribution with `\(P-1\)` degrees of freedom.
]

---


# A regression-based Diebold-Mariano test

.right-column[
In practice, the test of equal predictive ability can be applied within the framework of a regression model: `$$d(e_{t+h}) = \delta + \upsilon_{t+h} \hspace{.5in} t = R,\ldots,T-h.$$`

The null of equal predictive ability is equivalent of testing `\(H_0: \delta = 0\)`.

Because `\(d(e_{t+h})\)` may be serially correlated, an autocorrelation consistent standard errors should be used for inference.
]

---

# Predictive accuracy of El Nino forecasts

.right-column[
Are the forecasts from ECMWF statistically significantly more accurate than those from JMA (the next best model)?

As it turns out, the DM statistic is `\(-2.930\)`. Which means, we reject the null hypothesis of equal predictive accuracy.

So, yes, ECMWF forecasts are more accurate than those of JMA.
]

---


# Combining forecasts

.right-column[
By choosing the most accurate of the forecasts, we discard all others.

But other forecasts may not be completely useless. They could potentially contain information above and beyond of that of the most accurate forecast.

Thus, merely selecting the best model may be a sub-optimal strategy.

An optimal strategy may be using some information from all forecasts, i.e., *forecast combination*.
]

---


# Why might we combine?
	
.right-column[
Several factors support the idea of forecast combination:

- The concept is intuitively appealing;
- The method is computationally simple;
- The outcome is surprisingly good.
]

---


# How can we combine?

.right-column[
Consider two forecasting methods (or models), `\(i\)` and `\(j\)`, each respectively yielding `\(h\)`-step-ahead forecasts `\(y_{i,t+h|t}\)` and `\(y_{j,t+h|t}\)`, and the associated forecast errors `\(e_{i,t+h} = y_{t+h}-y_{i,t+h|t}\)` and `\(e_{j,t+h} = y_{t+h}-y_{j,t+h|t}\)`.

A combined forecast, `\(y_{c,t+h|t}\)`, is expressed as: `$$y_{c,t+h|t} = (1-w)y_{i,t+h|t} + w y_{j,t+h|t},$$` where `\(0 \leq w \leq 1\)` is a weight. Thus, a combined forecast is the weighted average of the two individual forecasts.
]

---


# Mean of a combined forecast error

.right-column[
A combined forecast error is the weighted average of the two individual forecast errors: `$$e_{c,t+h} = (1-w)e_{i,t+h} + w e_{j,t+h}$$`

The mean of a combined forecast error (under the assumption of forecast error unbiasedness) is zero: `$$\mathbb{E}\left(e_{c,t+h}\right) = \mathbb{E}\left[(1-w)e_{i,t+h} + w e_{j,t+h}\right] = 0$$`
]

---


# Variance of a combined forecast error

.right-column[
The variance of a combined forecast error is: `$$Var\left(e_{c,t+h}\right) = (1-w)^2 \sigma_i^2 + w^2  \sigma_j^2 + 2w(1-w)\rho\sigma_i\sigma_j,$$` where `\(\sigma_i\)` and `\(\sigma_j\)` are the standard deviations of the forecast errors from models `\(i\)` and `\(j\)`, and `\(\rho\)` is a correlation between these two forecast errors.
]

---


# The optimal weight

.right-column[
Taking the derivative of the variance of a combined forecast error, and equating it to zero yields an optimal weight (which minimizes the combined forecast error variance): `$$w^* = \frac{\sigma_i^2-\rho\sigma_i\sigma_j}{\sigma_i^2+\sigma_j^2-2\rho\sigma_i\sigma_j}$$`
]

---


# At least as efficient as the most efficient forecast

.right-column[
Substitute `\(w^*\)` in place of `\(w\)` in the formula for variance to obtain: `$$Var\left[e_{c,t+1}(w^*)\right] = \sigma_c^2(w^*) = \frac{\sigma_i^2\sigma_j^2(1-\rho^2)}{\sigma_i^2+\sigma_j^2-2\rho\sigma_i\sigma_j}$$`


As it turns out: `\(\sigma_c^2(w^*) \leq \min\{\sigma_i^2,\sigma_j^2\}\)`. That is, by combining forecasts we are not making things worse (so long as we use *optimal* weights).
]

---


# Combining equally accurate forecasts

.right-column[
*Assumption*: `\(\sigma_i = \sigma_j = \sigma\)`.

Suppose the individual forecasts are equally accurate, then the combined forecast error variance reduces to: `$$\sigma_c^2(w^*) = \frac{\sigma^2(1+\rho)}{2} \leq \sigma^2$$`

The equation shows there are diversification gains even when the forecasts are equally accurate (unless the forecasts are perfectly correlated, in which case there are no gains from combination).
]

---


# Combining uncorrelated forecasts

.right-column[
*Assumption*: `\(\rho=0\)`.

Suppose the forecast errors are uncorrelated, then the sample estimator of `\(w^*\)` is given by: `$$w^* = \frac{\sigma_i^2}{\sigma_i^2+\sigma_j^2} = \frac{\sigma_j^{-2}}{\sigma_i^{-2}+\sigma_j^{-2}}$$`

Thus, the weights attached to forecasts are inversely proportional to the variance of these forecasts.
]

---


# Combination equally accurate uncorrelated forecasts

.right-column[
*Assumption*: `\(\sigma_i = \sigma_j = \sigma\)` and `\(\rho=0\)`.

Suppose the individual forecasts are equally accurate and the forecast errors are uncorrelated, then the sample estimator of `\(w^*\)` reduces to `\(0.5\)`, resulting in the equal-weighted forecast combination: `$$y_{c,t+h|t} = 0.5y_{i,t+h|t} + 0.5y_{j,t+h|t}$$`

]

---


# Predictive accuracy of combined El Nino forecasts

.right-column[
Are the combined forecasts statistically significantly more accurate than those from ECMWF (the best model)?

When applying "equal weights", the DM statistic is `\(1.543\)`. Which means, we fail to reject the null hypothesis of equal predictive accuracy.

When applying "inversely proportional weights", the DM statistic is `\(2.062\)`. Which means, we reject the null hypothesis of equal predictive accuracy in favor of the combined forecast.
]

---


# The optimal weight in a regression setting

.right-column[
The optimal weight has a direct interpretation in a regression setting. 

Consider the combined forecast equation as: `$$y_{t+h} = (1-w)y_{i,t+h|t} + w y_{j,t+h|t} + \varepsilon_{t+h},$$` where `\(\varepsilon_{t+h}\equiv e_{c,t+h}\)`. 
]

---


# The optimal weight in a regression setting

.right-column[
We can re-arrange the equation so that: `$$e_{i,t+h} = w (y_{j,t+h|t}-y_{i,t+h|t}) + \varepsilon_{t+h},$$` or that: `$$e_{i,t+h} = w\left(e_{i,t+h}-e_{j,t+h}\right)+\varepsilon_{t+h},\;~~t=R,\ldots,R+P-1$$` where `\(w\)` is obtained by estimating a linear regression with an intercept restricted to zero.
]

---

# The optimal weight in a regression setting

.right-column[
Alternatively, we can estimate a variant of the combined forecast equation: `$$y_{t+h} = \alpha+\beta_i y_{i,t+h|t} + \beta_j y_{j,t+h|t} + \varepsilon_{t+h},$$` which relaxes the assumption of forecast unbiasedness, as well as of weights adding up to one or, indeed, of non-negative weights.
]

---


# Forecast encompassing

.right-column[
A special case of forecast combination is when `\(w=0\)`. Such an outcome (of the optimal weights) is known as forecast encompassing.

It is said that `\(y_{i,t+h|t}\)` encompasses `\(y_{j,t+h|t}\)`, when given that the former is available, the latter provides no additional useful information.
]

---


# Forecast encompassing in a regression setting

.right-column[
We can test for the forecast encompassing by regressing the realized value on individual forecasts: `$$y_{t+h} = \alpha + \beta_1 y_{i,t+h|t} + \beta_2 y_{j,t+h|t} + \varepsilon_{t+h},$$` and testing the null hypothesis that `\(\beta_2=0\)`, given that `\(\beta_1=1\)`. 
]

---


# Readings

.pull-left[
![](Art/todolist.png)
]

.pull-right[
Gonzalez-Rivera, Chapter 9 Sections 2 and 3
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>
<style>
.logo {
  background-image: url(forecasting-logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  bottom: .5em;
  left: 2em;
  width: 91px;
  height: 105px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
