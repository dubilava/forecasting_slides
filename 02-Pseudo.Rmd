---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 2: Generating and Evaluating Forecasts"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    includes:
      after_body: insert-logo.html
    nature:
      highlightStyle: github
      countIncrementalSlides: false
      ratio: '16:9'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 11, fig.height = 7)
```



```{r echo=FALSE, include=FALSE, message=FALSE}
library(data.table)
library(ggplot2)
```


# Forecast is a random variable

.pull-left[
![](Art/forecast.png)
]

.pull-right[
A forecast is a random variable which has some distribution and, thus, moments.

A simplest form of a forecast is *point forecast*.

A more complex but much more informative form of a forecast is *density forecast*.

A relatively less complex form of a forecast is *interval forecast*, which is the lower and upper percentiles of the forecast distribution.
]

---


# Forecast is a random variable

.right-column[
Consider a forecast, made in period $t$ for a future period $t+h$, where $h$ is forecast horizon.

A complete $h$-step-ahead forecast, $y_{t+h|t}$, can be fully summarized by the (conditional) distribution $F(y_{t+h}|\Omega_t)$ or the density $f(y_{t+h}|\Omega_t)$.

Both $F(y_{t+h}|\Omega_t)$ and $f(y_{t+h}|\Omega_t)$ summarize all the knowns and unknowns about the potential values of $y$ at time $t+h$, given the available knowledge at time $t$.
]

---


# Generating forecasts

.right-column[
Forecast accuracy should only be determined by considering how well a model performs on data not used in estimation.

But to assess forecast accuracy we need access to the data, typically from future time periods, that was not used in estimation.

This leads to the so-called 'pseudo forecasting' routine. 
]

---


# Pseudo forecasting routine

.right-column[
The pseudo forecasting routine involves slitting the available data into two segments referred to as 'in-sample' and 'out-of-sample'

- The in-sample segment of a series is also known as the 'estimation set' or the 'training set.'
- The out-of-sample segment of a series is also known as the 'hold-out set' or the 'test set.'
]

---


# Pseudo forecasting routine

.right-column[
Thus, we make the so-called 'genuine' forecasts using only the information from the estimation set, and assess the accuracy of these forecasts in an out-of-sample setting.

Because forecasting is often performed in a time series context, the estimation set typically predates the hold-out set. In non-dynamic settings such chronological ordering may not be necessary, however.
]

---


# Forecasting schemes

.right-column[
As we carry on the pseudo-forecasting routine, we update the information set by including the more recent observations. In so doing, we may choose to keep or discard the more distant observations, which leads to two possible ways of selecting and updating information sets.
- The *recursive* window routine uses a sequence of expanding windows to update model estimates and the information set.
- The *rolling* window routine uses a sequence of windows of the same size to update model estimates and the information set.
]

---

# Recursive vs. Rolling

.right-column[
The two pseudo-forecasting routines are technically very similar. But they can generate considerably differing sets of forecasts. 

The recursive scheme uses increasingly more data to generate successive forecasts. If the data-generating process does not change over time this may be a preferred scheme as it allows us to estimate the model parameters more precisely. 

But if the data-generating process changes over time, then the use of all available data may as well harm our forecastsâ€”they will be biased toward the historical past that is no longer relevant. In such instances, the rolling scheme may be a preferred alternative. 
]

---


# Forecast evaluation

.right-column[
To evaluate forecasts of a time series, $\{y_t\}$, with a total of $T$ observations, we divide the sample into two parts, the in-sample set with a total of $R$ observations, such that $R < T$ (typically, $R \approx 0.75T$), and the out-of-sample set. 

If we are interested in one-step-ahead forecast assessment, this way we will produce a sequence of forecasts: $\{y_{R+1|R},y_{R+2|{R+1}},\ldots,y_{T|{T-1}}\}$ for $\{Y_{R+1},Y_{R+2},\ldots,Y_{T}\}$.

Forecast errors, $e_{R+j} = y_{R+j} - y_{R+j|{R+j-1}}$, then can be computed for $j = 1,\ldots,T-R$.
]

---


# Measures of forecast accuracy

.right-column[
The most commonly applied accuracy measures are the mean absolute forecast error (MAFE) and the root mean squared forecast error (RMSFE):
$$\begin{aligned}
\text{MAFE}  = & \frac{1}{P}\sum_{i=1}^{P}|e_i|\\
\text{RMSFE} = & \sqrt{\frac{1}{P}\sum_{i=1}^{P}e_i^2}
\end{aligned}$$
where $P$ is the total number of out-of-sample forecasts.
]

---


# Measures of forecast accuracy

.right-column[
The MAFE assumes absolute loss, while the RMSFE assumes quadratic loss. 

The lower is the measure of the forecast accuracy, the better is the model or the method used in generating forecasts. 

Unlike some of the better-known in-sample goodness-of-fit measures (e.g., R-squared), these accuracy measures, on their own, have little meaning. They gain the meaning when we compare two (or more) models or methods of forecasting. 
]

---


# Measures of forecast adequacy

.right-column[
MAFE and RMSFE (and other similar measures of forecast accuracy) are *relative* measures designed to compare forecasts. These are the mean values (or monotonic transformations thereof) of some loss functions. 

But there also exist *absolute* measures designed to evaluate each forecast separately. These are measures of forecast *unbiasedness* and *efficiency*, together forming the measure of forecast adequacy (or *rationality*).
]

---


# Forecast unbiasedness

.right-column[
A forecast is unbiased if the forecast error is zero, on average. 

This condition mimics that of the residuals of a model. But the residuals are, by construction, guaranteed to be mean--zero. This is not the case with  forecast errors.
]

---


# Forecast unbiasedness

.right-column[
Consider a positively trending time series. And suppose, due to ignorance or incompetence, we did not account for the trend. Instead, we assumed that an appropriate model was the simple mean of the observed series. 

The forecasts from this model will likely underestimate the realized future values of the time series. That is, the forecasts will be biased.
]

---


# Forecast unbiasedness

.right-column[
```{r bias-trending}
knitr::include_graphics("figures/lecture2/trending.png")
```
]

---


# Forecast unbiasedness

.right-column[
```{r errors-trending}
knitr::include_graphics("figures/lecture2/errors_trending.png")
```
]

---


# Forecast unbiasedness

.right-column[
It is obvious the mean of these forecast errors is some positive value, which is considerably different from zero. This suggests an issue in the model used to generate forecasts.

Of course, we are well aware of the issue - in generating the forecasts, we have ignored the presence of a trend in the time series.
]

---


# Forecast unbiasedness

.right-column[
The bias, in this instance, contributes to the lack of accuracy of the applied model. The RMSFE, based on these 20 forecast errors, is $0.80$. Had there been no bias, the RMSFE, based on the bias-corrected 20 forecast errors, would have been $0.59$. 

Biased forecasts usually (albeit not necessarily) are less accurate than their unbiased counterparts.
]

---


# Properties of a forecast error

.right-column[
Forecast errors of a 'good' forecasting method will have the following properties:
- zero mean; otherwise, the forecasts are biased.
- no correlation with the forecasts; otherwise, there is information left that should be used in computing forecasts.
- no serial correlation among one-step-ahead forecast errors. Note that $k$-step-ahead forecasts, for $k>1$, can be, and usually are, serially correlated.

These properties, in effect, are testable hypotheses. 
]

---


# Forecast error diagnostics

.right-column[
## Unbiasedness

Testing $E(e_{t+h|t})=0$. Set up a regression: $$e_{t+h|t} = \alpha+\upsilon_{t+h} \hspace{.5in} t = R,\ldots,T-h,$$
where $R$ is the estimation window size, $T$ is the sample size, and $h$ is the forecast horizon length. The null of zero-mean forecast error is equivalent of testing $H_0: \alpha = 0$ in the OLS setting. For $h$-step-ahead forecast errors, when $h>1$, autocorrelation consistent standard errors should be used.
]

---


# Forecast error diagnostics

.right-column[
## Efficiency

Testing $Cov(e_{t+h|t},y_{t+h|t})=0$. Set up a regression: $$e_{t+h|t} = \alpha + \beta y_{t+h|t} + \upsilon_{t+h} \hspace{.5in} t = R,\ldots,T-h.$$ The null of forecast error independence of the information set is equivalent of testing $H_0: \beta = 0$ in the OLS setting. For $h$-step-ahead forecast errors, when $h>1$, autocorrelation consistent standard errors should be used.
]

---


# Forecast error diagnostics

.right-column[
## No Autocorrelation

Testing $Cov(e_{t+1|t},e_{t|t-1})=0$. Set up a regression: $$e_{t+1|t} = \alpha + \gamma e_{t|t-1} + \upsilon_{t+1} \hspace{.5in} t = R+1,\ldots,T-1.$$ The null of no forecast error autocorrelation is equivalent of testing $H_0: \gamma = 0$ in the OLS setting.
]

---



# Readings

.pull-left[
![](Art/todolist.png)
]

.pull-right[
Ubilava, [Chapter 3](https://davidubilava.com/forecasting/docs/features-of-time-series-data.html)

Gonzalez-Rivera, Chapter 4

Hyndman & Athanasopoulos, [1.7](https://otexts.com/fpp3/perspective.html),  [5.4](https://otexts.com/fpp3/diagnostics.html), [5.8](https://otexts.com/fpp3/accuracy.html)
]




