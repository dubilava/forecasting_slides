---
title: "Forecasting for Economics and Business"
subtitle: "Lecture 6: Vector Autoregression"
author: "David Ubilava"
date: "University of Sydney"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, style.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 11, fig.height = 7)
```


```{r echo=FALSE, include=FALSE, message=FALSE}
library(ggplot2)
```

# Vector Autoregressive Model

Economic variables are often (and usually) interrelated; for example,

- income affects consumption; 
- interest rates impact investment.

The dynamic linkages between two (or more) economic variables can be modeled in a *system of equations* setting, better known as a vector autoregressive (VAR) process

---

# Vector Autoregressive Model

To begin, consider a bivariate (two-equation) VAR of order 1, VAR(1). 

Let $\{X_{1,t}\}$ and $\{X_{2,t}\}$ be the stationary stochastic processes. A bivariate VAR(1), is then given by:
$$\begin{aligned}
x_{1,t} &= \alpha_1 + \pi_{11}x_{1,t-1} + \pi_{12}x_{2,t-1} + \varepsilon_{1,t} \\
x_{2,t} &= \alpha_2 + \pi_{21}x_{1,t-1} + \pi_{22}x_{2,t-1} + \varepsilon_{2,t}
\end{aligned}$$

where $\varepsilon_{1,t} \sim iid(0,\sigma_1^2)$ and $\varepsilon_{2,t} \sim iid(0,\sigma_2^2)$, and the two can be correlated, i.e., $Cov(\varepsilon_{1,t},\varepsilon_{2,t}) \neq 0$.

---

# Vector Autoregressive Model

To generalize, consider an $n$-dimensional VAR of order $p$, VAR(p), presented in matrix notation: $$\mathbf{x}_t = \mathbf{\alpha} + \Pi_1 \mathbf{x}_{t-1} + \ldots + \Pi_p \mathbf{x}_{t-p} + \mathbf{\varepsilon}_t,$$ where $\mathbf{x}_t = (x_{1,t},\ldots,x_{n,t})'$ is a vector of $n$ (potentially) related variables; $\mathbf{\varepsilon}_t = (\varepsilon_{1,t},\ldots,\varepsilon_{n,t})'$ is a vector of error terms, such that $E(\mathbf{\varepsilon}_t) = \mathbf{0}$, $E(\mathbf{\varepsilon}_t^{}\mathbf{\varepsilon}_t^{\prime}) = \Sigma$, and $E(\mathbf{\varepsilon}_{t}^{}\mathbf{\varepsilon}_{s \neq t}^{\prime}) = 0$. $\Pi_1,\ldots,\Pi_p$ are $n$-dimensional parameter matrices: 
$$\Pi_j = 
	    \left[ 
		\begin{array}{cccc} 
		\pi_{11j} & \pi_{12j} & \cdots &  \pi_{1nj} \\ 
		\pi_{21j} & \pi_{22j} & \cdots &  \pi_{2nj} \\  
		\vdots & \vdots & \ddots &  \vdots \\  
		\pi_{n1j} & \pi_{n2j} & \cdots &  \pi_{nnj} \\  
		\end{array} 
		\right],\;~~j=1,\ldots,p$$
		
---

# Vector Autoregressive Model

Substitute recursively in a bivariate VAR(1) to get: 
$$\begin{aligned}
\mathbf{x}_t &= \mathbf{\alpha} + \Pi_1 \mathbf{x}_{t-1}+ \mathbf{\varepsilon}_t \notag \\
&= \mathbf{\alpha} + \Pi_1\left(\mathbf{\alpha} + \Pi_1 \mathbf{x}_{t-2}+ \mathbf{\varepsilon}_{t-1}\right)+ \mathbf{\varepsilon}_t \notag \\
&= \left(I + \Pi_1\right)\mathbf{\alpha} + \Pi_1^2 \mathbf{x}_{t-2}+ \Pi_1 \mathbf{\varepsilon}_{t-1}+ \mathbf{\varepsilon}_t \notag \\
&\vdots  \notag \\
&= \left(I + \sum_{i=1}^{k-1}\Pi_1^i\right)\mathbf{\alpha} + \Pi_1^k \mathbf{x}_{t-k}+ \sum_{i=0}^{k-1}\Pi_1^{i} \mathbf{\varepsilon}_{t-i} 
\end{aligned}$$
As $k \to \infty$, the stable VAR(1) process converges to: $$\mathbf{x}_t = \left(I - \Pi_1\right)^{-1}\mathbf{\alpha} + \sum_{i=0}^{\infty}\Pi_1^{i} \mathbf{\varepsilon}_{t-i}$$

---

# Vector Autoregressive Model

The expectation of $\mathbf{x_t}$ will yield the unconditional mean: $$E(\mathbf{x}_t) \equiv \mathbf{\mu} = \left(I - \Pi_1\right)^{-1}\mathbf{\alpha}.$$ More generally, i.e., in the case of a multivariate VAR(p),
$$\mathbf{\mu} =\left(I - \Pi_1 - \cdots - \Pi_p\right)^{-1}\mathbf{\alpha}$$

---

# Vector Autoregressive Model

The general features of a vector autoregressive model are:

- Only lagged values of the dependent variables are considered as the right-hand-side variables.
  * Although, trend and seasonal variables might also be included in higher-frequency data analysis.
- Each equation has the same set of right-hand-side variables.
  * However, it is possible to impose different lag structure across the equations, especially when $p$ is relatively large. This is because the number of parameters increases very quickly with the number of lags or the number of variables in the system.
- The autregressive order, $p$, is the largest number of lags across all equations.

---

# Modeling Vector Autoregression

The autoregressive order, $p$, can be determined using system-wide information criteria:

$$\begin{aligned}
& AIC = \ln\left|\Sigma_{\varepsilon}\right| + \frac{2}{T}(pn^2+n) \\
& SIC = \ln\left|\Sigma_{\varepsilon}\right| + \frac{\ln{T}}{T}(pn^2+n)
\end{aligned}$$

where $\left|\Sigma_{\varepsilon}\right|$ is the determinant of the residual covariance matrix; $n$ is the number of equations, and $T$ is the total number of observations.

---

# Modeling Vector Autoregression

When each equation of VAR has the same regressors, the OLS can be applied to each equation individually to estimate the regression parameters - i.e., the estimation can be carried out on the equation-by-equation basis (otherwise, the method of maximum likelihood is to be applied).

When processes are covariance-stationarity, conventional t-tests and F-tests are applicable for hypotheses testing.

---

# Testing In-Sample Granger Causality

Consider a bivariate VAR(p): 
$$\begin{aligned}
x_{1,t} &= \alpha_1 + \pi_{111} x_{1,t-1} + \cdots + \pi_{11p} x_{1,t-p} \\
&+ \pi_{121} x_{2,t-1} + \cdots + \pi_{12p} x_{2,t-p} +\varepsilon_{1,t}  \\
x_{2,t} &= \alpha_1 + \pi_{211} x_{1,t-1} + \cdots + \pi_{21p} x_{1,t-p} \\
&+ \pi_{221} x_{2,t-1} + \cdots + \pi_{22p} x_{2,t-p} +\varepsilon_{2,t} 
\end{aligned}$$

It is said that:
- $\{X_2\}$ does not Granger cause $\{X_1\}$ if $\pi_{121}=\cdots=\pi_{12p}=0$
- $\{X_1\}$ does not Granger cause $\{X_2\}$ if $\pi_{211}=\cdots=\pi_{21p}=0$


---


# Forecasting with Multivariate Models

Consider, again, a bivariate VAR(1): 
$$\begin{aligned}
x_{1,t} &= \alpha_1 + \pi_{11} x_{1,t-1} + \pi_{12} x_{2,t-1} + \varepsilon_{1,t} \\
x_{2,t} &= \alpha_2 + \pi_{21} x_{1,t-1} + \pi_{22} x_{2,t-1} + \varepsilon_{2,t}
\end{aligned}$$

The optimal one-step-ahead forecasts are:
$$\begin{aligned}
x_{1,t+1|t} &= E(x_{1,t+1}|\Omega_t) = \alpha_1 + \pi_{11} x_{1,t} + \pi_{12} x_{2,t} \\
x_{2,t+1|t} &= E(x_{2,t+1}|\Omega_t) = \alpha_2 + \pi_{21} x_{1,t} + \pi_{22} x_{2,t}
\end{aligned}$$

---

# Forecasting with Multivariate Models

The one-step-ahead forecast errors are: 
$$\begin{aligned}
e_{1,t+1|t} &= x_{1,t+1} - x_{1,t+1|t} = \varepsilon_{1,t+1} \\
e_{2,t+1|t} &= x_{2,t+1} - x_{2,t+1|t} = \varepsilon_{2,t+1}
\end{aligned}$$

The one-step-ahead forecast variances are:
$$\begin{aligned}
\sigma_{1,t+1|t}^2 &= E(x_{1,t+1} - x_{1,t+1|t}|\Omega_t)^2 = E(\varepsilon_{1,t+1}^2) = \sigma_{1}^2 \\
\sigma_{2,t+1|t}^2 &= E(x_{2,t+1} - x_{2,t+1|t}|\Omega_t)^2 = E(\varepsilon_{2,t+1}^2) = \sigma_{2}^2
\end{aligned}$$

---

# Forecasting with Multivariate Models

The optimal two-step-ahead forecasts are:
$$\begin{aligned}
x_{1,t+2|t} &= E(x_{1,t+2}|\Omega_t) = \alpha_1 + \pi_{11} x_{1,t+1|t} + \pi_{12} x_{2,t+1|t} \\
x_{2,t+2|t} &= E(x_{2,t+2}|\Omega_t) = \alpha_2 + \pi_{21} x_{1,t+1|t} + \pi_{22} x_{2,t+1|t}
\end{aligned}$$

The two-step-ahead forecast errors are:
$$\begin{aligned}
e_{1,t+2|t} &= x_{1,t+2} - x_{1,t+2|t} = \pi_{11} e_{1,t+1|t} + \pi_{12} e_{2,t+1|t} + \varepsilon_{1,t+2} \\
e_{2,t+2|t} &= x_{2,t+2} - x_{2,t+2|t} = \pi_{21} e_{1,t+1|t} + \pi_{22} e_{2,t+1|t} + \varepsilon_{2,t+2}
\end{aligned}$$

---

# Forecasting with Multivariate Models

The two-step-ahead forecast variances are:
$$\begin{aligned}
\sigma_{1,t+2|t}^2 &= E(x_{1,t+2} - x_{1,t+2|t}|\Omega_t)^2 \\ 
&= \sigma_{1}^2(1+\pi_{11}^2) + \sigma_{2}^2\pi_{12}^2 + 2\pi_{11}\pi_{12} Cov(\varepsilon_{1},\varepsilon_{2})\\
\sigma_{2,t+2|t}^2 &= E(x_{2,t+2} - x_{2,t+2|t}|\Omega_t)^2 \\ 
&= \sigma_{2}^2(1+\pi_{22}^2) + \sigma_{1}^2\pi_{21}^2 + 2\pi_{21}\pi_{22} Cov(\varepsilon_{1},\varepsilon_{2})
\end{aligned}$$

---


# Out-of-Sample Granger Causality

The previously discussed (in sample) tests of causality in granger sense are frequently performed in practice, but the 'true spirit' of such test is to assess the ability of a variable to help predict another variable in an out-of-sample setting.

Consider restricted and unrestricted information sets:
$$\begin{aligned}
&\Omega_{t,r} \equiv \Omega_{t}(X_1) = \{x_{1,t},x_{1,t-1},\ldots\} \\
&\Omega_{t,u} \equiv \Omega_{t}(X_1,X_2) = \{x_{1,t},x_{1,t-1},\ldots,x_{2,t},x_{2,t-1},\ldots\}
\end{aligned}$$

Following Granger's definition of causality: $\{X_2\}$ is said to cause $\{X_1\}$ if $\sigma_{x_1}^2\left(\Omega_{t,u}\right) < \sigma_{x_1}^2\left(\Omega_{t,r}\right)$, meaning that we can better predict $X_1$ using all available information on $X_1$ and $X_2$, rather than that on $X_1$ only.

---

# Out-of-Sample Granger Causality

Let the forecasts based on each of the information sets be:
$$\begin{aligned}
	&x_{1,t+h|t,r} = E\left(x_{1,t+h}|\Omega_{t,r}\right) \\
	&x_{1,t+h|t,u} = E\left(x_{1,t+h}|\Omega_{t,u}\right)
\end{aligned}$$

For these forecasts, the corresponding forecast errors are:
$$\begin{aligned}
	& e_{1,t+h|t,r} = x_{1,t+h} - x_{1,t+h|t,r}\\
	& e_{1,t+h|t,u} = x_{1,t+h} - x_{1,t+h|t,u}
\end{aligned}$$

The out-of-sample forecast errors are then evaluated by comparing the loss functions based on these forecasts errors.

---

# Out-of-Sample Granger Causality

For example, assuming quadratic loss, and $P$ out-of-sample forecasts:
$$\begin{aligned}
RMSFE_{r} &= \sqrt{\frac{1}{P}\sum_{s=1}^{P}\left(e_{1,R+s|R-1+s,r}\right)^2} \\
RMSFE_{u} &= \sqrt{\frac{1}{P}\sum_{s=1}^{P}\left(e_{1,R+s|R-1+s,u}\right)^2}
\end{aligned}$$
where $R$ is the size of the (first) estimation window.

$\{X_2\}$ is said to cause *in Granger sense* $\{X_1\}$ if $RMSFE_{u} < RMSFE_{r}$.

---

# Readings

Hyndman & Athanasopoulos, [Sections from Chapter 12](https://otexts.com/fpp3/toolbox.html)

Gonzalez-Rivera, Chapter 11



